{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$ \\text{ Identifying Duplicate Listings }$$ \n",
    "><br/>The goal of this test is to propose a listings duplicate detection method. Ou datasets of listing contains 1427 listings each listing provides the following information:<br/>\n",
    ">* listing_id\n",
    ">* surface_m2\n",
    ">* description \n",
    ">* current_price\n",
    ">* city\n",
    ">* transaction_type\n",
    ">* item_type\n",
    ">* room_count\n",
    ">* floor\n",
    ">* floor_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools, functools\n",
    "import copy\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\text{ 1. Loading Data} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* df_listing: is the dataset containning 1427 listings\n",
    ">* df_dlisting: is a dataset containing the pairs of duplicate listings<br/> \n",
    "<br/>\n",
    "We flatten df_dlisting in order to have groups of duplicates instead of pairs, we also replace Nan value in df_listing with the value -1<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>surface_m2</th>\n",
       "      <th>description</th>\n",
       "      <th>room_count</th>\n",
       "      <th>floor</th>\n",
       "      <th>floor_count</th>\n",
       "      <th>current_price</th>\n",
       "      <th>city</th>\n",
       "      <th>city_zip</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>item_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121205222</td>\n",
       "      <td>57</td>\n",
       "      <td>*** CONFLANS SAINTE HONORINE - proche COMMERCE...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195000</td>\n",
       "      <td>Conflans-Sainte-Honorine</td>\n",
       "      <td>78700</td>\n",
       "      <td>sell</td>\n",
       "      <td>apartment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110085329</td>\n",
       "      <td>48</td>\n",
       "      <td>** AGENCE WEELODGE ** Dans le secteur de FIN D...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>157500</td>\n",
       "      <td>Conflans-Sainte-Honorine</td>\n",
       "      <td>78700</td>\n",
       "      <td>sell</td>\n",
       "      <td>apartment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122402493</td>\n",
       "      <td>53</td>\n",
       "      <td>*** CONFLANS SAINTE HONORINE - IDEAL 1ER ACHAT...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142000</td>\n",
       "      <td>Conflans-Sainte-Honorine</td>\n",
       "      <td>78700</td>\n",
       "      <td>sell</td>\n",
       "      <td>apartment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117274348</td>\n",
       "      <td>67</td>\n",
       "      <td>** AGENCE WEELODGE ** PROCHE GARE SNCF et CENT...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>185000</td>\n",
       "      <td>Conflans-Sainte-Honorine</td>\n",
       "      <td>78700</td>\n",
       "      <td>sell</td>\n",
       "      <td>apartment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123155187</td>\n",
       "      <td>75</td>\n",
       "      <td>** AGENCE WEELODGE CONFLANS ** A 10MIN DE LA G...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>189900</td>\n",
       "      <td>Conflans-Sainte-Honorine</td>\n",
       "      <td>78700</td>\n",
       "      <td>sell</td>\n",
       "      <td>apartment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>117949689</td>\n",
       "      <td>54</td>\n",
       "      <td>*** CONFLANS SAINTE HONORINE *** Situé à PROXI...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179500</td>\n",
       "      <td>Conflans-Sainte-Honorine</td>\n",
       "      <td>78700</td>\n",
       "      <td>sell</td>\n",
       "      <td>apartment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>118800823</td>\n",
       "      <td>62</td>\n",
       "      <td>** EXCLUSIVITÉ ** CONFLANS: Proche GARES RER/ ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>198000</td>\n",
       "      <td>Conflans-Sainte-Honorine</td>\n",
       "      <td>78700</td>\n",
       "      <td>sell</td>\n",
       "      <td>apartment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>118800825</td>\n",
       "      <td>55</td>\n",
       "      <td>CONFLANS: EMPLACEMENT IDÉAL ! Situé en plein c...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>179500</td>\n",
       "      <td>Conflans-Sainte-Honorine</td>\n",
       "      <td>78700</td>\n",
       "      <td>sell</td>\n",
       "      <td>apartment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>115945848</td>\n",
       "      <td>67</td>\n",
       "      <td>CONFLANS ***EXCLUSIVITÉ*** À 800m de la GARE S...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>169000</td>\n",
       "      <td>Conflans-Sainte-Honorine</td>\n",
       "      <td>78700</td>\n",
       "      <td>sell</td>\n",
       "      <td>apartment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>118789540</td>\n",
       "      <td>59</td>\n",
       "      <td>***RENOUVEAU***LA PERLE RARE UNIQUEMENT CHEZ W...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>258000</td>\n",
       "      <td>Conflans-Sainte-Honorine</td>\n",
       "      <td>78700</td>\n",
       "      <td>sell</td>\n",
       "      <td>apartment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1428 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      listing_id  surface_m2  \\\n",
       "0      121205222          57   \n",
       "1      110085329          48   \n",
       "2      122402493          53   \n",
       "3      117274348          67   \n",
       "4      123155187          75   \n",
       "...          ...         ...   \n",
       "1423   117949689          54   \n",
       "1424   118800823          62   \n",
       "1425   118800825          55   \n",
       "1426   115945848          67   \n",
       "1427   118789540          59   \n",
       "\n",
       "                                            description  room_count  floor  \\\n",
       "0     *** CONFLANS SAINTE HONORINE - proche COMMERCE...         4.0    3.0   \n",
       "1     ** AGENCE WEELODGE ** Dans le secteur de FIN D...         2.0    NaN   \n",
       "2     *** CONFLANS SAINTE HONORINE - IDEAL 1ER ACHAT...         3.0    2.0   \n",
       "3     ** AGENCE WEELODGE ** PROCHE GARE SNCF et CENT...         4.0    4.0   \n",
       "4     ** AGENCE WEELODGE CONFLANS ** A 10MIN DE LA G...         4.0    3.0   \n",
       "...                                                 ...         ...    ...   \n",
       "1423  *** CONFLANS SAINTE HONORINE *** Situé à PROXI...         3.0    1.0   \n",
       "1424  ** EXCLUSIVITÉ ** CONFLANS: Proche GARES RER/ ...         3.0    0.0   \n",
       "1425  CONFLANS: EMPLACEMENT IDÉAL ! Situé en plein c...         3.0    1.0   \n",
       "1426  CONFLANS ***EXCLUSIVITÉ*** À 800m de la GARE S...         4.0    0.0   \n",
       "1427  ***RENOUVEAU***LA PERLE RARE UNIQUEMENT CHEZ W...         3.0    NaN   \n",
       "\n",
       "      floor_count  current_price                      city  city_zip  \\\n",
       "0             NaN         195000  Conflans-Sainte-Honorine     78700   \n",
       "1             NaN         157500  Conflans-Sainte-Honorine     78700   \n",
       "2             NaN         142000  Conflans-Sainte-Honorine     78700   \n",
       "3             NaN         185000  Conflans-Sainte-Honorine     78700   \n",
       "4             4.0         189900  Conflans-Sainte-Honorine     78700   \n",
       "...           ...            ...                       ...       ...   \n",
       "1423          NaN         179500  Conflans-Sainte-Honorine     78700   \n",
       "1424          4.0         198000  Conflans-Sainte-Honorine     78700   \n",
       "1425          1.0         179500  Conflans-Sainte-Honorine     78700   \n",
       "1426          4.0         169000  Conflans-Sainte-Honorine     78700   \n",
       "1427          NaN         258000  Conflans-Sainte-Honorine     78700   \n",
       "\n",
       "     transaction_type  item_type  \n",
       "0                sell  apartment  \n",
       "1                sell  apartment  \n",
       "2                sell  apartment  \n",
       "3                sell  apartment  \n",
       "4                sell  apartment  \n",
       "...               ...        ...  \n",
       "1423             sell  apartment  \n",
       "1424             sell  apartment  \n",
       "1425             sell  apartment  \n",
       "1426             sell  apartment  \n",
       "1427             sell  apartment  \n",
       "\n",
       "[1428 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id_1</th>\n",
       "      <th>listing_id_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64721495</td>\n",
       "      <td>64728971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64721495</td>\n",
       "      <td>65459581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64728971</td>\n",
       "      <td>65459581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98430083</td>\n",
       "      <td>98429480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119279327</td>\n",
       "      <td>116630374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3579</th>\n",
       "      <td>121226460</td>\n",
       "      <td>121615906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3580</th>\n",
       "      <td>67292960</td>\n",
       "      <td>69521695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3581</th>\n",
       "      <td>67292960</td>\n",
       "      <td>68728687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3582</th>\n",
       "      <td>119227626</td>\n",
       "      <td>120836693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3583</th>\n",
       "      <td>119227626</td>\n",
       "      <td>122513025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3584 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      listing_id_1  listing_id_2\n",
       "0         64721495      64728971\n",
       "1         64721495      65459581\n",
       "2         64728971      65459581\n",
       "3         98430083      98429480\n",
       "4        119279327     116630374\n",
       "...            ...           ...\n",
       "3579     121226460     121615906\n",
       "3580      67292960      69521695\n",
       "3581      67292960      68728687\n",
       "3582     119227626     120836693\n",
       "3583     119227626     122513025\n",
       "\n",
       "[3584 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_dlisting = pd.read_csv(\"./ml_test_dataset/duplicate_listing.csv\")\n",
    "df_listing = pd.read_csv(\"./ml_test_dataset/listing.csv\")\n",
    "\n",
    "display(df_listing)\n",
    "display(df_dlisting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\text{ 1.2 Fill Nan Values} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> <br/> Only the columns **room-count**, **floor** and **floor_count** have nan values.<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing_id            0\n",
      "surface_m2            0\n",
      "description           0\n",
      "room_count            3\n",
      "floor               151\n",
      "floor_count         474\n",
      "current_price         0\n",
      "city                  0\n",
      "city_zip              0\n",
      "transaction_type      0\n",
      "item_type             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_listing.isna().sum())\n",
    "# Sort columns acoording to the number of Nans, this order will be used to group the listings \n",
    "sorted_columns = df_listing.isna().sum().sort_values(ascending = True).index\n",
    "# Fill Nan values with -1\n",
    "df_listing = df_listing.fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### $ \\text{ 1.3 Form groups from pairs of duplicates} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 277 groups of duplicates\n"
     ]
    }
   ],
   "source": [
    "# form groups of duplicates from pairs \n",
    "\n",
    "def partition(pred, iterable):\n",
    "    t1, t2 = itertools.tee(iterable)\n",
    "    return itertools.filterfalse(pred, t1), filter(pred, t2)\n",
    "\n",
    "groups = []\n",
    "list_ids = list(df_listing['listing_id'])\n",
    "for i in range(len(df_dlisting)):\n",
    "    a, b = df_dlisting.loc[i]\n",
    "    unrelated, related = partition(lambda group: any(aa == a or bb == b or aa == b or bb == a for aa, bb in group), groups)\n",
    "    groups = [*unrelated, sum(related, [(a, b)])]\n",
    "flaten_groups = []\n",
    "for grp in groups:\n",
    "    g = set([j for tuple in grp for j in tuple if j in list_ids])\n",
    "    if len(g)>1:\n",
    "        flaten_groups.append(list(g))\n",
    "print(f\"We have {len(flaten_groups)} groups of duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\text{2. Detect duplicates}$\n",
    "\n",
    "> <br/>In this section we will first detect the duplicates that match at 100\\% using pandas library, then we form groups from the remaining listings according to differents columns. Once the small groups are formed we will run a Deep Learning model to estimate the similarity between listings of the same group. <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\text{2.1 Detect 100\\% matching duplicates}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <br/>To Better match between the **descriptions** we will run a small text preprocessing to remove spaces and special caracters<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of description\n",
    "import re\n",
    "def text_processing(s):\n",
    "    if s != None:\n",
    "        s = s.lower()\n",
    "        s = s.replace('<br />', '')\n",
    "        s = re.sub('[.,;=:!@#$+<>-]', ' ', s)\n",
    "        s = \" \".join(s.split())\n",
    "    return s\n",
    "df_listing['description'] = df_listing['description'].map(lambda x: text_processing(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 removed listings\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates and save the ids of duplicated listings\n",
    "\n",
    "columns = list(df_listing.columns)\n",
    "df_listing = df_listing.fillna(-1)\n",
    "columns.remove(\"listing_id\")\n",
    "duplicates = df_listing.duplicated(subset=columns, keep=False)\n",
    "\n",
    "direct_matches = df_listing.loc[duplicates, :]\n",
    "# Groupby columns in order to get the duplicates in groups\n",
    "direct_matches_gb = direct_matches.groupby(by=columns, dropna=False)\n",
    "predicted_pairs = []\n",
    "for key, item in direct_matches_gb:\n",
    "    predicted_pairs.append(list(direct_matches_gb.get_group(key)['listing_id']))\n",
    "\n",
    "# Remove Duplicates and keeping only the first occurence\n",
    "new_df_listing = df_listing.drop_duplicates(subset=columns, keep='first')\n",
    "print(f'{len(df_listing)-len(new_df_listing)} removed listings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\text{2.2 Group by columns}$\n",
    "\n",
    "> <br/>After removing the obvious duplicates, we will groupby our dataset on the columns `['city', 'city_zip', 'item_type', 'room_count', 'floor', 'floor_count']`. We first start to group by the columns that don't have nan value. As for the columns that have nan values (previously replaced with -1), we remove the sub-group formed with the `key=-1` and add it to the other sub-group; since the value is missing, the listing could belong to any sub-group.<br/><br/>\n",
    "The column `['current_price']` is removed since the price of duplicates can be different. For the column `['surface_m2']` the duplicates can have a diffent surfface but within an interval of more or less 3 $m^2$<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city', 'city_zip', 'transaction_type', 'item_type', 'room_count', 'floor', 'floor_count']\n"
     ]
    }
   ],
   "source": [
    "rm_col = ['surface_m2', 'current_price', 'listing_id', 'description']\n",
    "columns = [c for c in sorted_columns if c not in rm_col]\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = copy.deepcopy(new_df_listing)\n",
    "grps = [df]\n",
    " \n",
    "for c in columns:\n",
    "    new_grps = []\n",
    "    for g in grps:\n",
    "            # Group By column c\n",
    "            group_by_df = g.groupby(by=c, dropna=False)\n",
    "            nan_group = None\n",
    "            keys = list(group_by_df.agg(lambda x: list(x)).index)\n",
    "            if -1 in keys:\n",
    "                nan_group = group_by_df.get_group(-1)\n",
    "            for key, item in group_by_df:\n",
    "                if key!=-1:\n",
    "                    if nan_group is None:\n",
    "                        new_grps.append(group_by_df.get_group(key))\n",
    "                    else:\n",
    "                        # concatenate with nan Group\n",
    "                        new_grps.append(pd.concat([group_by_df.get_group(key), nan_group]))\n",
    "                \n",
    "    grps = new_grps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_surface(group, df, v=3):\n",
    "    \"\"\"\n",
    "    Check if the surfaces of the group are in the same interval\n",
    "    \"\"\"\n",
    "    df_g = df.loc[df['listing_id'].isin(group)]\n",
    "    surfaces = pd.Series(df_g['surface_m2']).value_counts()\n",
    "    if len(surfaces)>1:\n",
    "        new_g = []\n",
    "        max_surface = surfaces.index[0]\n",
    "        for id in group: \n",
    "            id_surface = list(df_g.loc[df_g['listing_id']==id]['surface_m2'])[0]\n",
    "            if abs(max_surface-id_surface) <= v:\n",
    "                new_g.append(id)\n",
    "        return new_g\n",
    "    else:\n",
    "        return list(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\text{2.3 Sentence-Bert}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> <br/>The idea of Sentence-BERT is to modify the architecture of BERT into a Siamese network.\n",
    "The two sentences of a pair go through a BERT followed by a pooling operation to obtain the two\n",
    "embeddings u and v. Several methods can be used for the pooling step, although\n",
    "the researchers found that mean pooling worked better.<br/><br/>\n",
    "After the pooling is done, we have 2 embeddings: 1 for sentence A and 1 for sentence B. At inference\n",
    "the two embeddings are then compared using a cosine similarity function, which will output a similarity score for the two sentences. <br/><br/>\n",
    "\n",
    "![sentence-Bert](sbert.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Get pretrained Model\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "for g in grps:\n",
    "    g_embeddings = model.encode(list(g['description']))\n",
    "    for e in g_embeddings:\n",
    "            ids = np.array(g['listing_id'])\n",
    "            results = np.array((util.dot_score(e, g_embeddings)>0.9).squeeze(0))\n",
    "            duplicates = list(ids[results])\n",
    "            # ADD to duplicates\n",
    "            if len(duplicates)>1:\n",
    "                # Check if the surfaces are correct\n",
    "                duplicates = process_surface(duplicates, new_df_listing)\n",
    "                for pairs in predicted_pairs:\n",
    "                    if any(item in duplicates for item in pairs):\n",
    "                        # Remove pairs from duplicates\n",
    "                        predicted_pairs.remove(pairs)\n",
    "                        duplicates = list(set(list(duplicates)+list(pairs)))\n",
    "                predicted_pairs.append(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\text{2.4 Results}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP (Number of duplicates that have been predicted): 1078\n",
      "TN (Number of duplicates that have not been predicted): 152\n",
      "FP (Number of wrong prediction): 12\n",
      "Precision: 0.99\n",
      "Recall: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Precision and Recall considering individual listings (not pairs)\n",
    "\n",
    "groud_truth = set(sum([list(l) for l in flaten_groups if len(l)>1], []))\n",
    "prediction = set(sum(predicted_pairs, []))\n",
    "intersection = groud_truth.intersection(prediction)\n",
    "print('TP (Number of duplicates that have been predicted):',len(intersection))\n",
    "print('TN (Number of duplicates that have not been predicted):',(len(groud_truth) - len(intersection)))\n",
    "print('FP (Number of wrong prediction):',(len(prediction) - len(groud_truth.intersection(prediction))))\n",
    "print('Precision: {:.2f}'.format(len(intersection)/len(prediction)))\n",
    "print('Recall: {:.2f}'.format(len(intersection)/len(groud_truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.89\n",
      "Recall: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Precision and Recall considering pairs/couples of listings\n",
    "\n",
    "all_predicted_pairs = set(sum([list(combinations(sorted(set(pair)), 2)) for pair  in predicted_pairs], []))\n",
    "all_ground_truth_pairs = set(sum([list(combinations(sorted(set(pair)), 2)) for pair in flaten_groups ], []))\n",
    "intersection = all_ground_truth_pairs.intersection(all_predicted_pairs)\n",
    "\n",
    "print('Precision: {:.2f}'.format(len(intersection)/len(all_predicted_pairs)))\n",
    "print('Recall: {:.2f}'.format(len(intersection)/len(all_ground_truth_pairs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Some errors in the df_dlisting dataset (duplicates pairs dataset) : 74 couples have different **floor** numbers but labeled as duplicates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\text{2.5 Ideas to improve the solution}$\n",
    "\n",
    "* Finetune the sentence-Bert model on our data to make it more specific to our task.\n",
    "\n",
    "* Use a model to detect [\n",
    "Image Duplicates & Near Duplicates.](https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/image-search/Image_Duplicates.ipynb)\n",
    "\n",
    "* Make sure to have correctly labeled datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('condaenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263f1b304a1c299581de447ea272a3862daa476db20457714cd8514b4053ef84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
